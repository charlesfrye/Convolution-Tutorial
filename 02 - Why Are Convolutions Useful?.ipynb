{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viva la Convolution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution is a basic mathematical operation -- in a very real way, it's only slightly less fundamental than addition and multiplication! Because of its fundamental nature, convolution arises repeatedly in both theoretical and applied contexts.\n",
    "\n",
    "This notebook focuses on offering examples of applications of convolution, in particular to signal processing and neuroscience. If you want to learn more about the theory of convolutions, check out [this blog post by Chris Olah](http://colah.github.io/posts/2014-12-Groups-Convolution/) or the [first notebook in this tutorial](01%20-%20What's%20a%20Convolution%3F.ipynb). Since the material here is more technical, much of the nitty-gritty code is made visible to the readers, unlike in the other notebooks in this tutorial, in hopes they might find it useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import rcParams\n",
    "\n",
    "%matplotlib inline\n",
    "plt.xkcd(); rcParams['font.family'].append('Bitstream Vera Sans')\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy import misc\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from IPython.display import Audio\n",
    "\n",
    "import scipy.signal\n",
    "\n",
    "import utils.util as util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution on Audio Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code blocks load an audio file, define some convolutions, and then apply those convolutions to the audio file. If you have a .wav file lying around on your computer, just change the `filename` in the first line of code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'data/Kira_Training1.wav' #your wavfile here!\n",
    "\n",
    "samplerate,waveform = wavfile.read(filename) \n",
    "plt.figure(figsize=(12,2))\n",
    "plt.plot(waveform); util.cleanPlot(plt.gca()); \n",
    "plt.title(\"The dog jumped over the fence\");\n",
    "Audio('data/Kira_Training1.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define convolutions using something called a \"kernel\". If we view our convolution operation as a function that takes a signal as input and produce a signal as output, the kernel represents the output signal for an input signal that's just a single data point with the value 1, presented at time 0. We get the total output for a signal with many points at different times and with different values by adding together the kernel responses to each point. These responses are scaled by the value of the signal at that datapoint and shifted by how far into the signal the datapoint it.\n",
    "\n",
    "If you'd like to know more, check out the [first notebook in this tutorial](01%20-%20What's%20a%20Convolution%3F.ipynb) for a walkthrough of the convolution operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define Our Kernels\n",
    "pad = 5; padding = [0]*pad\n",
    "difference = padding+[-1/2,0,1/2]+padding;\n",
    "average = padding+[1/3,1/3,1/3]+padding\n",
    "                      \n",
    "echoTime = 0.25\n",
    "echoGapLength = samplerate*echoTime\n",
    "echo = padding+[1]+[0]*int(echoGapLength)+[1]+padding\n",
    "\n",
    "kernels = [difference,average,echo]\n",
    "kernelNames = ['high-pass/difference','low-pass/moving average','echo']\n",
    "numKernels = len(kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Our Kernels  \n",
    "util.kernelsPlot(kernels,kernelNames)\n",
    "plt.suptitle('Kernels!',fontsize=20,weight='bold',y=1.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"echo\" kernel is easy to interpret -- when you put in a single point, it first responds by spitting that point back out, then waits a few seconds, and then spits the point back out again. If you do this for all of the points in your audio signal, adding the outputs on top of each other with appropriate delays, you get out two copies of the signal layered on top each other, with one of them delayed in time. Sounds like an echo to me!\n",
    "\n",
    "Two of the convolutions act as frequency filters -- they remove either the low-frequency components (letting the high frequencies pass) or the high frequency components (letting the low frequencies pass). Discovering this from their kernels is kinda hard. (Optional section ahead!) If you know about [Fourier transforms](http://www.neurotheory.columbia.edu/Ken/math-notes/math-notes-4.pdf) and the [convolution theorem](https://www.youtube.com/watch?v=a0IdGLczoAA), then try Fourier transforming them and looking at the frequency plots. If you don't want to do the Fourier transform yourself, you can look up the Fourier transform of the boxcar function.\n",
    "\n",
    "If you're unfamiliar with thinking about signals in frequency space, you can think of these kernels as the \"difference\" and \"moving average\" kernels instead. Can you see why that might be the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "signals = [waveform]*(numKernels+1)\n",
    "signalNames = ['Original','High-Pass Filtered','Low-Pass Filtered','Echoed']\n",
    "\n",
    "# Convolve Signals With Kernels n times\n",
    "n = 10\n",
    "for idx,kernel in enumerate(kernels):\n",
    "    for _ in range(n):\n",
    "        signals[idx+1] = np.convolve(signals[idx+1],kernel)\n",
    "\n",
    "    \n",
    "# Plot those bad boys\n",
    "plt.figure(figsize=(12,2*(numKernels+1))); \n",
    "ct = 1\n",
    "\n",
    "for signal,title in zip(signals,signalNames):\n",
    "    plt.subplot(len(signals),1,ct); ct +=1\n",
    "    plt.plot(signal); util.cleanPlot(plt.gca())\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell lets you listen to the results, instead of just looking at them. Just change the `idx` variable. If any of the signals won't play, try reducing the `n` variable in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Listen to each signal by indexing into signals with idx below\n",
    "# 0 for the original, 1 for the high-pass filtered one, etc.\n",
    "idx = 3\n",
    "Audio(signals[idx],rate=samplerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution on Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply convolutions in more than one dimension. In two dimensions, we can view the results as images. Not only that, but we can use convolutions on images to get new images, as we did when we applied the echo kernel to our sound, or to extract useful information, as when we used the difference kernel to extract the high frequency content of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load in an image that comes with Scipy\n",
    "rawIm = misc.face(gray=True)[:500,300:]\n",
    "\n",
    "#alternatively: upload your own image!\n",
    "#rawIm = scipy.ndimage.io.imread('data/burtreynolds.jpg', mode='L')\n",
    "\n",
    "# pre-processing\n",
    "im = (rawIm - np.mean(rawIm))/np.std(rawIm)\n",
    "im = scipy.ndimage.gaussian_filter(im,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(im,cmap='Greys_r',interpolation='none'); util.removeAxes(plt.gca())\n",
    "plt.title('an image is just a 2-D signal!');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the audio signal, we were mostly interested in using convolution to generate new audio signals. Let's use a convolution to extract some information from our image, namely: where are the edges?\n",
    "\n",
    "We do this by defining kernels that look like tiny edges, then sliding them over our image. At each point, we plot the output of our kernel. This is just another way of describing the same operation we were performing above, where we made scaled copies of the kernel and added them together.\n",
    "\n",
    "One important difference is that the kernels we've defined will be mostly 0 or close to 0 (gray in the images below), and will only be large (positive or negative) in places where an edge of the right orientation is present. This makes them \"edge detectors\". Before, our kernels were usually close to the value of the input signal, so they weren't very good detectors of anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define some edge-detecting image kernels\n",
    "\n",
    "## make sure they add up/integrate to 0\n",
    "## make sure their length/l2 norm is 1\n",
    "\n",
    "i=1/np.sqrt(9)\n",
    "vEdgeDetection = [[i,-2*i,i],\n",
    "                  [i,-2*i,i],\n",
    "                  [i,-2*i,i]];\n",
    "\n",
    "hEdgeDetection = [[i,i,i],\n",
    "                  [-2*i,-2*i,-2*i],\n",
    "                  [i,i,i]];\n",
    "\n",
    "diagEdgeDetection =  [[i, i,-2*i],\n",
    "                      [i,-2*i, i],\n",
    "                      [-2*i,i, i,]]\n",
    "\n",
    "kernels = [vEdgeDetection,hEdgeDetection,diagEdgeDetection]\n",
    "kernelNames = [\"Vertical Edge Detector\",\"Horiztonal Edge Detector\",\"45 Degree Edge Detector\"]\n",
    "numKernels = len(kernels)\n",
    "\n",
    "signals = [im]\n",
    "signalNames = [\"Original\",\"Vertical Edges\",\"Horiztonal Edges\",\"45 Degree Edges\"]\n",
    "\n",
    "plt.figure(figsize=(12,4));\n",
    "\n",
    "for idx,(kernel,name) in enumerate(zip(kernels,kernelNames)):\n",
    "    signals.append(scipy.signal.convolve2d(signals[0],kernel))\n",
    "    plt.subplot(1,numKernels,idx+1); plt.title(name)\n",
    "    plt.imshow(kernel,interpolation='nearest',cmap=\"Greys_r\"); util.removeAxes(plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16)); numSignals = len(signals)\n",
    "cmaps = ['Greys_r','Greys_r','Greys_r','Greys_r'];\n",
    "\n",
    "for idx,(signal,name) in enumerate(zip(signals,signalNames)):\n",
    "    plt.subplot(2,numSignals//2,idx+1); plt.title(name)\n",
    "    plt.imshow(signal,cmap=cmaps[idx],interpolation='nearest'); util.removeAxes(plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contemporary machine learning terminology, the outputs of the convolutions are called *feature maps* -- they are like map laid over the image that tells us where certain features, like edges, are, much like a burrito shop map might tell us where the burrito shops are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution and Neuroscience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking signals in and extracting information from them is one of the primary functions of the nervous system, so it should come as no surprise that convolutions appear as models for neural activity.\n",
    "\n",
    "Of course, convolutions are pretty restricted, as far as signal transformations go (for mathy types, they are *linear* and *invariant to translations*), and we know neurons are capable of some pretty complicated transformations -- like extracting the spoken words from an audio stream, or turning an image into a series of ideas (like you're doing now!).\n",
    "\n",
    "We can make our convolutions more complicated if we allow some kind of non-convolutional transformation afterwards (for mathletes -- a *non-linear* transformation). One example might be the transformation between the total input to a neuron to its firing rate. A very simple model would say that when input is below a threshold value, firing rate is 0, and then firing rate rises linearly as input increases above that value (this is the response function of a [leaky integrator neuron](http://charlesfrye.github.io/FoundationalNeuroscience/25/)). The graph (which appears below) looks a bit like a hockey stick.\n",
    "\n",
    "So our simple neurons act as follows: they are a linear filter of some part of the visual field, and they take their output and push it through a non-linearity. If we have a whole bunch of them with the same filter, but scattered over the input space, then they define a convolution with that filter as a kernel, plus a non-linearity at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeCenterSurround():\n",
    "    N = 8/np.sqrt(72); n = 1/np.sqrt(72)\n",
    "    centerSurround =  [[0,0,0,0,0],\n",
    "                        [0,-n,-n,-n,0],\n",
    "                        [0,-n,N,-n,0],\n",
    "                        [0,-n,-n,-n,0],\n",
    "                        [0,0,0,0,0]]\n",
    "    centerSurround = centerSurround/np.sum(np.square(centerSurround))\n",
    "    return centerSurround\n",
    "\n",
    "kernels = [makeCenterSurround()]\n",
    "kernels.append(np.multiply(kernels[0],-1))\n",
    "\n",
    "plt.figure(figsize=(12,4)); plt.subplot(1,2,1)\n",
    "plt.imshow(kernels[0],cmap='Greys_r'); util.removeAxes(plt.gca()); \n",
    "plt.title('ON-Ganglion Cell Receptive Field');\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(-10,11),np.multiply(list(range(-10,11)),np.asarray([[0]*11+[1]*10])).T,zorder=10)\n",
    "plt.xlim([-10,10]); plt.ylim([-0.5,10])\n",
    "util.removeAxes(plt.gca())\n",
    "util.addAxis(plt.gca(),'horizontal');\n",
    "plt.title('Firing Rate Function: \"Rectified Linear\"');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've ever worked with real neurons, you're probably screaming right now that *real neurons* are way more complicated than that! As true as that is, it turns out you can get a lot of mileage out of this simple model. For one, it's a first step at [explaining perceptual invariance](http://charlesfrye.github.io/FoundationalNeuroscience/09/). For another, it's the core of the idea of a [convolutional neural network](http://colah.github.io/posts/2014-07-Conv-Nets-Modular/), which happens to be our current best technique for extracting information from images. It is also a major component of the algorithm that Google used to beat top human players at the ancient game of Go!\n",
    "\n",
    "In the code blocks below, I generate an example model based on one of the best-studied neural-populations the ON- and OFF-center ganglion cells of the retina. Check out [this blog post by yrs truly](http://charlesfrye.github.io/FoundationalNeuroscience/50/) for more on this circuit, including the biological and information-theoretic underpinnings of the receptive field shape.\n",
    "\n",
    "The resulting image doesn't seem that impressive, but it has [far fewer active neurons](http://charlesfrye.github.io/FoundationalNeuroscience/48/) (note that zero corresponds to black in the below processed images) and has [less redundant information](http://charlesfrye.github.io/FoundationalNeuroscience/51/) than a na√Øve code -- useful properties for a nervous system that has limited energy and space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "signals = [im]\n",
    "signalNames = [\"Original\",\"ON-Ganglion Cell Feature Map\",\"OFF-Ganglion Cell Feature Map\"]\n",
    "\n",
    "for idx,kernel in enumerate(kernels):\n",
    "    signals.append(scipy.signal.convolve2d(signals[0],kernel))\n",
    "    signals[-1] = np.multiply(signals[-1],np.greater(signals[-1],0))\n",
    "\n",
    "numSignals = len(signals)\n",
    "\n",
    "plt.figure(figsize=(6,18));\n",
    "for idx,(signal,name) in enumerate(zip(signals,signalNames)):\n",
    "    plt.subplot(numSignals,1,idx+1); plt.title(name)\n",
    "    plt.imshow(signal,cmap='Greys_r',interpolation='nearest'); util.removeAxes(plt.gca())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:binderbase]",
   "language": "python",
   "name": "conda-env-binderbase-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
