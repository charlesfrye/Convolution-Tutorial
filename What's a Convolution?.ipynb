{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions Aren't Convoluted!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution is a basic mathematical operation -- in a very real way, it's only slightly less fundamental than addition and multiplication\\*! Because of its fundamental nature, convolution arises repeatedly in both theoretical and applied contexts.\n",
    "\n",
    "This notebook focuses on the mathematical foundations of convolution. If you're interested in the applications of convolution, check out the [other notebook in this folder](Why%20Are%20Convolutions%20Useful%3F.ipynb).\n",
    "\n",
    "Below, I assume pretty minimal math background. There is a very elegant view of convolutions that comes from the theory of Fourier transforms and linear algebra. If you're interested, check out [this nice exposition of that point of view](http://www.neurotheory.columbia.edu/Ken/math-notes/math-notes-4.pdf), from Kenneth Miller of Columbia, which does a good job developing the approach gently. There are even loads of neuroscience examples!\n",
    "\n",
    "\\* For the mathematically inclined: convolutions are defined whenever you have a set with a binary operation -- that means groups, monoids, and even categories! Check out [this blog post by Chris Olah](http://colah.github.io/posts/2014-12-Groups-Convolution/) for more on that front, or for a very nice introduction to groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import rcParams\n",
    "#matplotlib.use(\"TkAgg\")\n",
    "%matplotlib inline\n",
    "plt.xkcd();\n",
    "rcParams['font.sans-serif'] = ['Comic Sans','TSCu_Comic']\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from IPython.display import Audio\n",
    "\n",
    "import scipy.signal\n",
    "\n",
    "import plotFunks as pF\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "import base64\n",
    "\n",
    "import scipy.signal\n",
    "\n",
    "import plotFunks as pF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotSignal(signal,signalName):\n",
    "    plt.figure(figsize=(16,2));\n",
    "    plt.plot(signal,'-o',color='k');\n",
    "    pF.cleanPlot(plt.gca()); \n",
    "    plt.xlim(-len(signal)/10,len(signal)+len(signal)/10);\n",
    "    pF.addAxis(plt.gca(),'horizontal');\n",
    "    plt.title(signalName);\n",
    "\n",
    "def plotSignalAsDelta(signal,signalName,color='r'):\n",
    "    plt.figure(figsize=(16,4)); plt.subplot(2,1,1)\n",
    "    plt.plot(signal,'-o',color='k');\n",
    "    pF.cleanPlot(plt.gca()); \n",
    "    plt.xlim(-len(signal)/10,len(signal)+len(signal)/10);\n",
    "    pF.addAxis(plt.gca(),'horizontal');\n",
    "    plt.title(signalName);\n",
    "    plt.subplot(2,1,2)\n",
    "    deltaPlot(signal,color=color);\n",
    "    pF.cleanPlot(plt.gca()); \n",
    "    plt.xlim(-len(signal)/10,len(signal)+len(signal)/10);\n",
    "    pF.addAxis(plt.gca(),'horizontal');\n",
    "    plt.title('also ' +signalName);\n",
    "        \n",
    "def deltaPlot(inp,color='r'):\n",
    "    plt.scatter(np.arange(0,len(inp)),inp,\n",
    "                linewidth=0,marker='o',s=36,color=color,\n",
    "                zorder=10); \n",
    "    plt.vlines(np.arange(0,len(inp)),0,inp,)\n",
    "\n",
    "def plotKronecker():\n",
    "    pad=[0,0]; padLen = len(pad)\n",
    "    deltaPlot(pad+[1]+pad,color='b')\n",
    "    pF.cleanPlot(plt.gca());\n",
    "    plt.xlim(-padLen/10,2*padLen+1+padLen/10)\n",
    "    pF.addAxis(plt.gca(),'horizontal');\n",
    "    plt.title('the delta function')\n",
    "    \n",
    "def kernelsPlot(kernels,kernelNames):\n",
    "    numKernels = len(kernels)\n",
    "    plt.figure(figsize=(16,4));\n",
    "    \n",
    "    for idx,(kernel,name) in enumerate(zip(kernels,kernelNames)):\n",
    "        plt.subplot(1,numKernels,idx+1)\n",
    "        deltaPlot(kernel)\n",
    "        pF.cleanPlot(plt.gca()); \n",
    "        plt.xlim(-len(kernel)/10,len(kernel)+len(kernel)/10);\n",
    "        pF.addAxis(plt.gca(),'horizontal');\n",
    "        plt.title(name);\n",
    "        \n",
    "def convolutionPlot(signals,signalName,kernels,kernelNames):\n",
    "    for idx,(signal,kernel,kernelName) in enumerate(zip(signals[1:],kernels,kernelNames)):\n",
    "        plt.figure(figsize=(16,4));\n",
    "        \n",
    "        plt.subplot(1,3,1)\n",
    "        #Plot the original signal for reference\n",
    "        deltaPlot(signals[0],color='blue');\n",
    "        plt.title(signalName); plt.ylim(-1,1.5); \n",
    "        plt.xlim(-len(signals[0])/10,len(signals[0])+len(signals[0])/10); \n",
    "        pF.cleanPlot(plt.gca()); pF.addAxis(plt.gca(),'horizontal')\n",
    "    \n",
    "        plt.subplot(1,3,2)\n",
    "        #Plot the kernel for reference\n",
    "        deltaPlot(kernel)\n",
    "        plt.title(kernelName); plt.ylim(-1,1.5); \n",
    "        plt.xlim(-len(kernel)/10,len(kernel)+len(kernel)/10); \n",
    "        pF.cleanPlot(plt.gca()); pF.addAxis(plt.gca(),'horizontal')\n",
    "    \n",
    "        plt.subplot(1,3,3)\n",
    "        #Plot convolved signal\n",
    "        outName = signalName+'*'+kernelName\n",
    "        sCf = deltaPlot(signal,color='purple'); plt.ylim(-1,1.5); \n",
    "        plt.xlim(-len(signal)/10,len(signal)+len(signal)/10);  \n",
    "        pF.cleanPlot(plt.gca()); pF.addAxis(plt.gca(),'horizontal')\n",
    "        plt.title(outName);\n",
    "\n",
    "def probabilityPlot(ax,locs,edge,labels):\n",
    "    ax.set_ylim([0,1]); ax.set_xlim([locs[0]-edge,locs[1]+edge]);\n",
    "    ax.xaxis.set_ticklabels('');\n",
    "    ax.xaxis.set_ticks(locs); ax.xaxis.set_ticklabels(labels)\n",
    "    ax.yaxis.set_ticks([0,0.5,1]);\n",
    "    ax.tick_params(axis='x',top='off')\n",
    "    ax.tick_params(axis='y',right='off')\n",
    "    plt.ylabel('Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML-based animation display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from http://jakevdp.github.io/blog/2013/05/12/embedding-matplotlib-animations/\n",
    "\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "VIDEO_TAG = \"\"\"<video controls>\n",
    " <source src=\"data:video/x-m4v;base64,{0}\" type=\"video/mp4\">\n",
    " Your browser does not support the video tag.\n",
    "</video>\"\"\"\n",
    "\n",
    "def anim_to_html(anim,fps=1):\n",
    "    if not hasattr(anim, '_encoded_video'):\n",
    "        with NamedTemporaryFile(suffix='.mp4') as f:\n",
    "            anim.save(f.name, fps=fps, \n",
    "                      extra_args=['-vcodec', 'libx264', '-pix_fmt', 'yuv420p'])\n",
    "            video = open(f.name, \"rb\").read()\n",
    "        anim._encoded_video = base64.b64encode(video).decode('utf-8')\n",
    "    \n",
    "    return VIDEO_TAG.format(anim._encoded_video)\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "def display_animation(anim,fps=1):\n",
    "    plt.close(anim._fig)\n",
    "    return HTML(anim_to_html(anim,fps=fps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Understanding Signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the traditional view, convolutions arise when we want to understand how very simple systems respond when signals pass through them. In order to make this idea concrete, we need to define \"simple systems\" and \"signals\".\n",
    "\n",
    "Discrete signals are, fundamentally, just indexed collections of numbers, also known as arrays. When we have a signal in time, a signal is an array of numbers. When that signal is distributed in space, like an image, it is a 2-D array of numbers.\n",
    "\n",
    "Below, we generate a signal using a [random walk](https://en.wikipedia.org/wiki/Random_walk). Random walks are used to model everything from the stock market to the motion of atoms. They represent the simplest form of an [auto-correlated](https://en.wikipedia.org/wiki/Autocorrelation) signal, so they're a nice modest step up from [white noise](https://en.wikipedia.org/wiki/White_noise). We generate a random walk by adding together samples from a Gaussian distribution -- the value at time `t` is just the running total of the sum of `t` samples.\n",
    "\n",
    "Because this signal is random, it can be helpful to come back and re-run the cell below in order to get more examples later -- some examples will be better than others for illustrating points about convolution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def randomWalk(tMax=1,sigma=1,eps=0.1):\n",
    "    signal=[0]; scaleFactor = np.sqrt(eps)\n",
    "    for t in np.arange(0,tMax,eps):\n",
    "        signal.append(signal[-1]+np.random.normal(0,sigma*scaleFactor))\n",
    "    return np.asarray(signal[1:])\n",
    "\n",
    "signals = [randomWalk(tMax=0.1,eps=0.01)]\n",
    "signal = signals[0]\n",
    "signalName = 's'\n",
    "\n",
    "plotSignal(signal,signalName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our signal is just an array, we can think of it as a collection of points. The zeroth element of the array is a point at $t = 0$, the next is a point at $t = 1$, etc. The height of the point is determined by the number in the array.\n",
    "\n",
    "Put another way, we can break a signal of length $N$ down into $N$ components. If we were to represent the component at the timepoint $i$ as a function $e_i$, it would look like:\n",
    "\n",
    "$$\n",
    "e_i(t) = 1 \\text{ if } t = i \\\\\n",
    "e_i(t) = 0 \\text{ otherwise }\n",
    "$$\n",
    "\n",
    "This function is also called a \"Kronecker's Delta\", \"the delta function\", or the \"unit impulse\". If we were to draw it, it might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotKronecker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct our signal from these components as follows: we multiply each $e_i$ by the signal $s$. The elements that are $0$ stay zero, while the element that is $1$ now has the value $s(i)$. We then add all $N$ of the $e_i$ together, and the result is our original signal.\n",
    "\n",
    "If you've taken linear algebra, this process may sound familiar to you -- it's exactly the way that we construct a vector out of a set of basis vectors. This is a deep connection. Just as the canonical, or usual, basis set for a two-dimensional vector space is the set $\\left\\{[1, \\ 0], [0,\\ 1]\\right\\}$, one common basis set for signals is the set of all Kronecker delta functions, each at a different point in time or space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotSignalAsDelta(signals[0],'s',color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Simple Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For us, a simple transformation is one whose response to a signal is just the sum of its responses to all of the components in the signal and which doesn't depend on time. In mathematical terms, it is *linear* and *translation-invariant*. Since the components of our signals are just single points at different times, this means we can learn everything we want to learn about a simple transformation by seeing what it does to a single point at a single time. \n",
    "\n",
    "This response is called a *kernel* or an *impulse response*. If we put in two points at once, the output is just two kernels stacked on top of one another -- note that this is the same thing as multiplying by 2. If we put in one point, then wait one time step and put in a second point, we get two copies of the kernel, separated by one time step.\n",
    "\n",
    "Let's take a look at some simple transformations. We start by defining and plotting their kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Our Kernels\n",
    "pad = 15; padding = [0]*pad\n",
    "delta = [1]+padding\n",
    "delay = padding+delta\n",
    "echo = [1]+padding+[1]+padding\n",
    "\n",
    "kernels = [delta,delay,echo]\n",
    "kernelNames = ['f',\"f'\",\"(f+f')\"]\n",
    "numKernels = len(kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Our Kernels  \n",
    "kernelsPlot(kernels,kernelNames)\n",
    "plt.suptitle('Kernels!',fontsize=20,weight='bold',y=1.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets apply these kernels to our \"unit impulse\" -- one point. Notice what comes out: it's just the kernel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simpleSignal = [1]\n",
    "simpleSignals = [simpleSignal]\n",
    "\n",
    "for kernel in kernels:\n",
    "    simpleSignals.append(np.convolve(simpleSignal,kernel))\n",
    "\n",
    "convolutionPlot(simpleSignals,'just one point',kernels,kernelNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"unit\" in \"unit impulse\" means \"having height 1\". What happens if we scale our unit impulse so that it has a different height?\n",
    "\n",
    "The response is just a scaled version of the kernel, just as when we put in two points at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simpleSignal = [1/2]\n",
    "simpleSignals = [simpleSignal]\n",
    "\n",
    "for kernel in kernels:\n",
    "    simpleSignals.append(np.convolve(simpleSignal,kernel))\n",
    "\n",
    "convolutionPlot(simpleSignals,'just one point',kernels,kernelNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the third filter's kernel is just the sum of the first two kernels. The response of the third filter is also a sum: it's the sum of the responses to the first two kernels.\n",
    "\n",
    "This is also true if we put two signals in: the response to a sum of inputs is just the sum of the responses to the individual inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twoPointSignal = [1,0,1,0,0,0]\n",
    "twoPointSignals = [twoPointSignal]\n",
    "\n",
    "for kernel in kernels:\n",
    "    twoPointSignals.append(np.convolve(twoPointSignal,kernel))\n",
    "\n",
    "convolutionPlot(twoPointSignals,'two points',kernels,kernelNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we have a more complicated signal, what we need to do to get the response to that signal is combine scaled, time-shifted copies of the kernel.\n",
    "\n",
    "Put more formally: any signal can be broken up into scaled delta functions at different times, and the response of our simple transformation to a scaled delta function at some time $t$ is just a scaled copy of the kernel starting at that time $t$.\n",
    "\n",
    "Let's put that into mathematical terms. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "response_{filter}(signal) =& \\sum_t response_{filter}(signal(t)) \\\\\n",
    "    =& \\sum_t response_{filter}(impulse(t)) \\cdot signal(t)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let's simplify our notation: we'll call the signal $s$ and the response of the filter $r_f$. We'll call the response to an impulse $k$. That makes our statement above:\n",
    "\n",
    "$$\n",
    "r_{f}(s) = \\sum_t k \\cdot s(t)\n",
    "$$\n",
    "\n",
    "This gives us the overall response, but what if we wanted to know the response at a particular time $t$? In order to do that, we have to keep track of all the possible contributions to the response at that time. That is, the response at $t$ will include the beginning of the response to the point at time $t$ plus the ongoing responses to the points at time $t-1$, at time $t-2$, and so on. We'll call that time difference $\\Delta$. \n",
    "\n",
    "And what is the value of the response at $t$ to a point at $t-\\Delta$? If $\\Delta$ is $0$, then it's the zeroth\\* element of the kernel, scaled by $s(t)$. If $\\Delta$ is $1$, then it's the first element of the kernel, scaled by $s(t-1)$. If $\\Delta$ is $2$, then it's the second element of the kernel, scaled by $s(t-2)$.\n",
    "\n",
    "There's an obvious pattern here: the response at $t$ to a point at $t-\\Delta$ is just the $\\Delta$th element of the kernel times the value of the signal at $t-\\Delta$.\n",
    "\n",
    "We should give a name to $t-\\Delta$. The traditional one is $\\tau$.\n",
    "\n",
    "To reiterate: the response to a point at time $\\tau$ is going to be a $s(\\tau)$-scaled copy of the kernel response. At any given time $t$ after $\\tau$, there will usually be several copies stacked on top of each other, each of them at a different point in their response, depending on how long after $\\tau$ the time $t$ is. This time difference is $\\Delta$.\n",
    "\n",
    "Let's write that out mathematically:\n",
    "\n",
    "$$\n",
    "r_{f}(s)(t) = \\sum_{\\tau+\\Delta = t} s(\\tau) \\cdot r(\\Delta)\n",
    "$$\n",
    "\n",
    "where summing over $\\tau+\\Delta=t$ means summing over all pairs of $\\tau$ and $\\Delta$ that sum to $t$. This mathematical expression just says what we said above: the response of a filter to a signal is a group of time-shifted kernels scaled by the past values of the signal. This expression just narrows that down to a single point.\n",
    "\n",
    "The mathematical expression above is exactly a convolution! It's not usually written that way, so for the benefit of anyone who has seen convolutions before, let's make some notation changes. First, we'll use the normal symbols: $f$ and $g$ instead of $r_f$ and $s$, and put a $*$ between them instead of putting parentheses. The result looks like\n",
    "\n",
    "$$\n",
    "g*f(t) = \\sum_{\\tau+\\Delta = t} g(\\tau) \\cdot f(\\Delta)\n",
    "$$\n",
    "\n",
    "It's also standard to use $\\tau$ by itself, making use of the fact that $\\Delta=t-\\tau$:\n",
    "\n",
    "$$\n",
    "g*f(t) = \\sum_{\\tau} g(\\tau)\\cdot f(t-\\tau)\n",
    "$$\n",
    "\n",
    "This is the more familiar expression of convolution, but I think it hides more than it shows. It's meant to evoke a dot product between the signal and a \"flipped around\" version of the kernel, but that ends up deeply confusing people. There's nothing \"flipped around\" about the kernel at all -- it's just that the later elements of the signal are contributing the earliest part of their responses. That is, as we look backwards in the signal, we find components that have made it further through their repsonse.\n",
    "\n",
    "\\* Using Python notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution for Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernels we used above were somewhat contrived: they mostly just repeated the signal. What if we wanted to do something more useful?\n",
    "\n",
    "Below, we define two more useful kernels: one that takes the difference between two points, and another that takes their average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define our filter kernels\n",
    "pad = 1;  padding = [0]*pad\n",
    "difference = padding+[1/2,0,-1/2]+padding;\n",
    "average = padding+[1/2,1/2]+padding\n",
    "\n",
    "kernels = [difference,average]\n",
    "kernelNames = ['difference','average']\n",
    "numKernels = len(kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Our Kernels\n",
    "kernelsPlot(kernels,kernelNames)    \n",
    "plt.suptitle('More Kernels!',fontsize=20,weight='bold',y=1.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `average` kernel has two points, both at height $1/2$. Let's plug that definition into our convolution expression:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "r_{f}(s)(t) &= \\sum_{\\tau+\\Delta = t} s(\\tau)\\cdot r(\\Delta) \\\\ \\\\\n",
    "            &= s(t)\\cdot r(0) + s(t-1)\\cdot r(1) \\\\ \\\\\n",
    "            &= s(t)\\cdot 1/2 + s(t-1)\\cdot 1/2 \\\\ \\\\\n",
    "            &= \\frac{s(t) + s(t-1)}{2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Notice that the final line is an expression for the average of two points. When we convolve this kernel with a signal, we get a *moving average*. How would we do a three-point moving average? What about an $N$-point moving average?\n",
    "\n",
    "The `difference` kernel also has two points, but one is at height $1/2$ and the other is at height $-1/2$, and there's one at height $0$ in between. Let's plug that definition into our convolution expression:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "r_{f}(s)(t) &= \\sum_{\\tau+\\Delta = t} s(\\tau)\\cdot r(\\Delta) \\\\ \\\\\n",
    "            &= s(t)\\cdot r(0) + s(t-1)*r(1) + s(t-2)\\cdot r(2) \\\\ \\\\\n",
    "            &= s(t)\\cdot 1/2 + s(t-2)\\cdot -1/2 \\\\ \\\\\n",
    "            &= \\frac{s(t) - s(t-2)}{2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Calculus aficionados will recognize that last line: it's the definition of a derivative! It is the change in the signal divided by the change in time -- \"the rise over the run\". Convolving with this kernel gives us the first derivative of our signal -- though usually, we call it a *difference* to remind ourselves that we aren't dealing with infinite numbers of points. What do you think might happen if we applied this kernel repeatedly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we use our kernels on the signal that we generated at the beginning of the section. You might be familiar with frequency-based analysis of signals. Can you see what our `difference` and `average` filters do to the signal in terms of frequnecy? Try generating new signals by rerunning the code block under **Understanding Signals**. It can be especially helpful to make longer ones by increasing the variable `tMax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use Our Kernels\n",
    "signals = [signals[0]]\n",
    "for kernel in kernels:\n",
    "    signals.append(np.convolve(signals[0],kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "convolutionPlot(signals,signalName,kernels,kernelNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution and Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But convolutions are much more than a simple way of expressing what simple transformations do to signals. In fact, they arise whenever we need to keep track of multiple possible contributions to a given value.\n",
    "\n",
    "Say we want to know the probability of getting exactly two heads in three coin tosses.\n",
    "First, we need to know the probability that the coin lands heads up. That plot appears below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pmfCoin = np.asarray([1/2,1/2])\n",
    "\n",
    "plt.figure(); locs = [0,2]; edge = 4\n",
    "labels = ['0H','1H']\n",
    "plt.bar(locs,pmfCoin,align='center');\n",
    "\n",
    "probabilityPlot(plt.gca(),locs,edge,labels)\n",
    "plt.suptitle(\"One Coin Flip\",size=24,weight='bold',y=1.);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $1H$ refers to getting a head and $0H$ refers to getting zero heads (aka a tails).\n",
    "\n",
    "What are the possible ways we can get two heads in three coin tosses? A table appears below.\n",
    "\n",
    "\n",
    "| First toss | Second Toss | Third Toss |\n",
    "|:----------:|:-----------:|:----------:|\n",
    "|      H     |      H      | T          |\n",
    "|      H     |      T      | H          |\n",
    "|      T     |      H      | H          |\n",
    "\n",
    "\n",
    "How do we compute the probability of each of those outcomes? For an individual coin flip, the probability of a head or a tail is $1/2$. The probability of any pair $HH$, $HT$, etc. is $1/2*1/2 = 1/4$. When we look at combined events, all we have to do is multiply the probabilities.\n",
    "\n",
    "And what is the overall probability that we get two heads? We just need to add up all the probabilities of the individual ways to get two heads. \n",
    "\n",
    "To write that out mathematically, we split our result into two parts: the first two tosses, and the third toss. We'll call those components $A$ and $B$, and call our result $C$. In order to figure out the overall probability of our result $C$, we need to add up all the probabilities of combinations $A$ and $B$ that give us $C$. In our case, those combinations would be:\n",
    "\n",
    "|  A | B |\n",
    "|:--:|:-:|\n",
    "| HH | T |\n",
    "| HT | H |\n",
    "| TH | H |\n",
    "\n",
    "We can find the probability of any given combination by multiplying together the probabilities of its components $A$ and $B$, but to find the probability of our outcome $C$, we need to take all combinations into account:\n",
    "\n",
    "$$\n",
    "p(C) = \\sum_{A+B=C} p(A)*p(B)\n",
    "$$\n",
    "\n",
    "Now doesn't that formula look familiar! It's a convolution, where the two functions being convolved are the probability functions of $A$ and $B$!\n",
    "\n",
    "Note that this applies for any number of repetitions. If we want to know how likely it is to get any particular number $k$ of heads in some number of coin tosses $n$, we just need to look at the $n$th convolution of the coin flip probability distribution with itself.\n",
    "\n",
    "The code block below generates an animated version of this process, showing you the result of each convolution in turn. Feel free to change the `pmfCoin` above (I suggest `[1/6]*6`, which gives the probability distribution for a six-sided coin, also known as a \"die\") or change the number of `iters` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pmfs = [pmfCoin]; #kernel = [1/2,1/2]\n",
    "iters = 25; #change me to get different numbers of flips\n",
    "mx = iters\n",
    "locs = list(range(mx+2))\n",
    "extendedPMF = np.hstack([pmfs[0],[0]*(mx+2-len(pmfs[0]))])\n",
    "edge = 2\n",
    "fig = plt.figure(figsize=(12,8)); pmfAx = plt.subplot(111);\n",
    "pmfBars = pmfAx.bar(locs,extendedPMF,align='center')\n",
    "labels = [str(n)+\"H\" for n in range(mx+1)]\n",
    "\n",
    "probabilityPlot(plt.gca(),locs,edge,labels)\n",
    "plt.suptitle(\"A Series of \"+str(iters)+\" Coin Flips\",size=24,weight='bold',y=1.)\n",
    "\n",
    "def init():\n",
    "    return\n",
    "\n",
    "def animate(_,pmfs):\n",
    "    [pmfBars[idx].set_height(h)\n",
    "         for idx,h in enumerate(pmfs[-1])]\n",
    "    pmfs.append(np.convolve(pmfs[-1],pmfs[0])) #Convolution!\n",
    "    return\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                                    fargs=[pmfs], frames=iters, \n",
    "                                    interval=2)\n",
    "\n",
    "display_animation(anim,fps=10) #change the FramesPerSecond here, for longer or shorter videos"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
