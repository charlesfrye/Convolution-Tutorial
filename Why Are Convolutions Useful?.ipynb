{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viva la Convolution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution is a basic mathematical operation -- in a very real way, it's only slightly less fundamental than addition and multiplication! Because of its fundamental nature, convolution arises repeatedly in both theoretical and applied contexts.\n",
    "\n",
    "This notebook focuses on offering examples of applications of convolution, in particular to signal processing and neuroscience. If you want to learn more about the theory of convolutions, check out [this blog post by Chris Olah](http://colah.github.io/posts/2014-12-Groups-Convolution/) or the [other notebook in this folder](What's%20a%20Convolution%3F.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import rcParams\n",
    "#matplotlib.use(\"TkAgg\")\n",
    "%matplotlib inline\n",
    "plt.xkcd();\n",
    "rcParams['font.sans-serif'] = ['Comic Sans','TSCu_Comic']\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy import misc\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from IPython.display import Audio\n",
    "\n",
    "import scipy.signal\n",
    "\n",
    "import plotFunks as pF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kernelsPlot(kernels,kernelNames):\n",
    "    plt.figure(figsize=(16,4)); numKernels= len(kernels)\n",
    "    for idx,(kernel,kernelName) in enumerate(zip(kernels,kernelNames)):\n",
    "        plt.subplot(1,numKernels,idx+1)\n",
    "        plotKernel(kernel)\n",
    "        plt.title(kernelName)\n",
    "        plt.ylim(-1,1.5); plt.xlim(-len(kernel)/10,len(kernel)+len(kernel)/10); \n",
    "        pF.cleanPlot(plt.gca()); pF.addAxis(plt.gca(),'horizontal')\n",
    "\n",
    "def plotKernel(kernel):\n",
    "    plt.scatter(np.arange(0,len(kernel)),kernel,\n",
    "                linewidth=0,marker='o',s=36,color='r',\n",
    "                zorder=10); \n",
    "    plt.vlines(np.arange(0,len(kernel)),0,kernel,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution on Audio Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code blocks load an audio file, define some convolutions, and then apply those convolutions to the audio file. If you have a .wav file lying around on your computer, just change the `filename` in the first line of code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'Kira_Training1.wav' #your wavfile here!\n",
    "\n",
    "samplerate,waveform = wavfile.read(filename) \n",
    "plt.figure(figsize=(12,2))\n",
    "plt.plot(waveform); pF.cleanPlot(plt.gca()); \n",
    "plt.title(\"The dog jumped over the fence\");\n",
    "Audio('Kira_Training1.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define convolutions using something called a \"kernel\". If we view our convolution operation as a function that takes a signal as input and produce a signal as output, the kernel represents the output signal for an input signal that's just a single data point with the value 1, presented at time 0. We get the total output for a signal with many points at different times and with different values by adding together the kernel responses to each point. These responses are scaled by the value of the signal at that datapoint and shifted by how far into the signal the datapoint it.\n",
    "\n",
    "If you'd like to know more, check out the [other notebook in this folder](What's%20a%20Convolution%3F.ipynb) for a walkthrough of the convolution operation.\n",
    "\n",
    "The \"echo\" kernel is easy to interpret in these terms -- when you put in a single point, it first responds by spitting that point back out, then waits a few seconds, and then spits the point back out again. If you do this for all of the points in your audio signal, adding the outputs on top of each other with appropriate delays, you get out two copies of the signal layered on top each other, with one of them delayed in time. Sounds like an echo to me!\n",
    "\n",
    "Two of the convolutions act as frequency filters -- they remove either the low-frequency components (letting the high frequencies pass) or the high frequency components (letting the low frequencies pass). Discovering this from their kernels is kinda hard. (Optional section ahead!) If you know about [Fourier transforms](http://www.neurotheory.columbia.edu/Ken/math-notes/math-notes-4.pdf) and the [convolution theorem](https://www.youtube.com/watch?v=a0IdGLczoAA), then try Fourier transforming them and looking at the frequency plots. If you don't want to do the Fourier transform yourself, you can look up the Fourier transform of the boxcar function.\n",
    "\n",
    "If you're unfamiliar with thinking about signals in frequency space, you can think of these kernels as the \"difference\" and \"moving average\" kernels instead. Can you see why that might be the case from the plots below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define Our Kernels\n",
    "pad = 5; padding = [0]*pad\n",
    "difference = padding+[-1/2,0,1/2]+padding;\n",
    "average = padding+[1/3,1/3,1/3]+padding\n",
    "                      \n",
    "echoTime = 0.25\n",
    "echoGapLength = samplerate*echoTime\n",
    "echo = padding+[1]+[0]*int(echoGapLength)+[1]+padding\n",
    "\n",
    "kernels = [difference,average,echo]\n",
    "kernelNames = ['high-pass/difference','low-pass/moving average','echo']\n",
    "numKernels = len(kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Our Kernels  \n",
    "kernelsPlot(kernels,kernelNames)\n",
    "plt.suptitle('Kernels!',fontsize=20,weight='bold',y=1.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "signals = [waveform]*(numKernels+1)\n",
    "signalNames = ['Original','High-Pass Filtered','Low-Pass Filtered','Echoed']\n",
    "\n",
    "# Convolve Signals With Kernels n times\n",
    "n = 10\n",
    "for idx,kernel in enumerate(kernels):\n",
    "    for _ in range(n):\n",
    "        signals[idx+1] = np.convolve(signals[idx+1],kernel)\n",
    "\n",
    "    \n",
    "# Plot those bad boys\n",
    "plt.figure(figsize=(12,2*(numKernels+1))); \n",
    "ct = 1\n",
    "\n",
    "for signal,title in zip(signals,signalNames):\n",
    "    plt.subplot(len(signals),1,ct); ct +=1\n",
    "    plt.plot(signal); pF.cleanPlot(plt.gca())\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell lets you listen to the results, instead of just looking at them. Just change the `idx` variable. If any of the signals won't play, try reducing the `n` variable in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Listen to each signal by indexing into signals with idx below\n",
    "# 0 for the original, 1 for the high-pass filtered one, etc.\n",
    "idx = 3\n",
    "Audio(signals[idx],rate=samplerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution on Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply convolutions in more than one dimension. In two dimensions, we can view the results as images. Not only that, but we can use convolutions on images to get new images, as we did when we applied the echo kernel to our sound, or to extract useful information, as when we used the difference kernel to extract the high frequency content of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawIm = misc.face(gray=True)\n",
    "\n",
    "im = (rawIm - np.mean(rawIm))/np.std(rawIm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(im,cmap='Greys_r',interpolation='none'); pF.removeAxes(plt.gca())\n",
    "plt.title('an image is just a 2-D signal!');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the audio signal, we were mostly interested in using convolution to generate new audio signals. Let's use a convolution to extract some information from our image, namely: where are the edges?\n",
    "\n",
    "We do this by defining kernels that look like tiny edges, then sliding them over our image. At each point, we plot the output of our kernel. This is just another way of describing the same operation we were performing above, where we made scaled copies of the kernel and added them together.\n",
    "\n",
    "One important difference is that the kernels we've defined will be mostly 0 or close to 0, and will only be large in places where an edge of the right orientation is present. This makes them \"edge detectors\". Before, our kernels were usually close to the value of the input signal, so they weren't very good detectors of anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define some edge-detecting image kernels\n",
    "i=1/np.sqrt(9)\n",
    "vEdgeDetection = [[i,-2*i,i],\n",
    "                  [i,-2*i,i],\n",
    "                  [i,-2*i,i]];\n",
    "\n",
    "hEdgeDetection = list(map(list, zip(*vEdgeDetection)))\n",
    "\n",
    "diagEdgeDetection =  [[i, i,-2*i],\n",
    "                      [i,-2*i, i],\n",
    "                      [-2*i,i, i,]]\n",
    "\n",
    "kernels = [vEdgeDetection,hEdgeDetection,diagEdgeDetection]\n",
    "kernelNames = [\"Vertical Edge Detector\",\"Horiztonal Edge Detector\",\"45 Degree Edge Detector\"]\n",
    "numKernels = len(kernels)\n",
    "\n",
    "signals = [im]\n",
    "signalNames = [\"Original\",\"Vertical Edges\",\"Horiztonal Edges\",\"45 Degree Edges\"]\n",
    "\n",
    "plt.figure(figsize=(12,4));\n",
    "\n",
    "for idx,(kernel,name) in enumerate(zip(kernels,kernelNames)):\n",
    "    signals.append(np.abs(scipy.signal.convolve2d(signals[0],kernel)))\n",
    "    plt.subplot(1,numKernels,idx+1); plt.title(name)\n",
    "    plt.imshow(kernel,interpolation='nearest',cmap=\"Greys_r\"); pF.removeAxes(plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16)); numSignals = len(signals)\n",
    "cmaps = ['Greys_r','Greys_r','Greys_r','Greys_r'];\n",
    "\n",
    "for idx,(signal,name) in enumerate(zip(signals,signalNames)):\n",
    "    plt.subplot(2,numSignals//2,idx+1); plt.title(name)\n",
    "    plt.imshow(signal,cmap=cmaps[idx],interpolation='nearest'); pF.removeAxes(plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution and Neuroscience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking signals in and extracting information from them is one of the primary functions of the nervous system, so it should come as no surprise that convolutions appear as models for neural activity.\n",
    "\n",
    "Of course, convolutions are pretty restricted, as far as signal transformations go (for mathy types, they are *linear* and *invariant to translations*), and we know neurons are capable of some pretty complicated transformations -- like extracting the spoken words from an audio stream, or turning an image into a series of ideas (like you're doing now!).\n",
    "\n",
    "We can make our convolutions more complicated if we allow some kind of non-convolutional transformation afterwards (for mathletes -- a *non-linear* transformation). One example might be the transformation between the total input to a neuron to its firing rate. A very simple model would say that when input is below a threshold value, firing rate is 0, and then firing rate rises linearly as input increases above that value (this is the response function of a [leaky integrator neuron](http://charlesfrye.github.io/FoundationalNeuroscience/25/)). The graph (which appears below) looks a bit like a hockey stick.\n",
    "\n",
    "So our simple neurons act as follows: they are a linear filter of some part of the visual field, and they take their output and push it through a non-linearity. If we have a whole bunch of them with the same filter, but scattered over the input space, then they define a convolution with that filter as a kernel, plus a non-linearity at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeCenterSurround(N):\n",
    "    n = N//8\n",
    "    return [[0,0,0,0,0],[0,-n,-n,-n,0],[0,-n,N,-n,0],[0,-n,-n,-n,0],[0,0,0,0,0]]\n",
    "\n",
    "intensities = [8] # ACHTUNG!: should be == 0 mod 8\n",
    "kernels = []\n",
    "\n",
    "for intensity in intensities:\n",
    "    kernels.append(makeCenterSurround(intensity))\n",
    "\n",
    "kernels.append(np.multiply(kernels[0],-1))\n",
    "\n",
    "plt.figure(figsize=(12,4)); plt.subplot(1,2,1)\n",
    "plt.imshow(kernels[0],cmap='Greys_r'); pF.removeAxes(plt.gca()); \n",
    "plt.title('ON-Ganglion Cell Receptive Field');\n",
    "plt.figtext(0.48,0.5,'=>',size='x-large',weight='bold')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(-10,11),np.multiply(list(range(-10,11)),np.asarray([[0]*11+[1]*10])).T,zorder=10)\n",
    "plt.xlim([-10,10]); plt.ylim([-0.5,10])\n",
    "pF.removeAxes(plt.gca())\n",
    "pF.addAxis(plt.gca(),'horizontal');\n",
    "plt.title('Firing Rate Function: \"Rectified Linear\"');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've ever worked with real neurons, you're probably screaming right now that *real neurons* are way more complicated than that! As true as that is, it turns out you can get a lot of mileage out of this simple model. For one, it's a first step at [explaining perceptual invariance](http://charlesfrye.github.io/FoundationalNeuroscience/09/). For another, it's the core of the idea of a [convolutional neural network](http://colah.github.io/posts/2014-07-Conv-Nets-Modular/), which happens to be our current best technique for extracting information from images. It is also a major component of the algorithm that Google used to beat the world champion at the ancient game of Go!\n",
    "\n",
    "In the code blocks below, I generate an example model based on one of the best-studied neural-populations the ON- and OFF-center ganglion cells of the retina. Check out [this blog post by yrs truly](http://charlesfrye.github.io/FoundationalNeuroscience/50/) for more on this circuit, including the biological and information-theoretic underpinnings of the receptive field shape.\n",
    "\n",
    "The resulting image doesn't seem that impressive, but it has [far fewer active neurons](http://charlesfrye.github.io/FoundationalNeuroscience/48/) and has [less redundant information](http://charlesfrye.github.io/FoundationalNeuroscience/51/) than a naïve code -- useful properties for a nervous system that has limited energy and space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "signals = [im]\n",
    "signalNames = [\"Original\",\"ON-Ganglion Cell Code\",\"OFF-Ganglion Cell Code\"]\n",
    "\n",
    "for idx,kernel in enumerate(kernels):\n",
    "    signals.append(scipy.signal.convolve2d(signals[0],kernel))\n",
    "    signals[-1] = np.multiply(signals[-1],np.greater(signals[-1],0))\n",
    "\n",
    "numSignals = len(signals)\n",
    "#numPlots = numSignals+1\n",
    "\n",
    "plt.figure(figsize=(6,18));\n",
    "for idx,(signal,name) in enumerate(zip(signals,signalNames)):\n",
    "    plt.subplot(numSignals,1,idx+1); plt.title(name)\n",
    "    plt.imshow(signal,cmap='Greys_r',interpolation='nearest'); pF.removeAxes(plt.gca())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
